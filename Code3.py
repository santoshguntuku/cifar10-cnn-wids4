# -*- coding: utf-8 -*-
"""23B2158_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEfjzQ8O2xJOA-oo--0MCOeDQY3-HNtV

# Assignment 3
# Convolutional Neural Networks Assignment
### Instructions:
1. Fill in the missing parts of the code wherever you see 'YOUR CODE HERE'.
2. Submit the completed notebook with outputs.
3. Provide explanations as comments where necessary

In this assignment, we aim to explore the concept of Convolutional Neural Networks (CNNs) and their suitability for image classification tasks. We will understand the significance of automatic feature learning in image classification and how CNNs excel in utilizing spatial relationships within images.

### A basic CNN architecture

CNNs are a class of deep learning models designed to automatically and adaptively learn spatial hierarchies of features from input data, particularly images. They are characterized by their ability to learn directly from pixel data with minimal preprocessing. A CNN employs spatial convolution for processing. This operation calculates a sum of products between pixel values and a set of kernel weights, applied at each spatial location in the input image. The result at each location is a scalar value, analogous to the output of a neuron in a fully connected neural network. Adding a bias and applying an activation function completes the analogy with fully connected networks.

A typical CNN architecture consists of several layers:

Convolutional Layer: These layers apply convolution operations to the input data, which involves sliding a small filter (kernel) over the input to detect patterns and features.

Pooling Layer: Pooling layers reduce the spatial dimensions of the input by taking the maximum or average value within a small window. This helps reduce the computational load and focuses on the most important features.

Fully Connected Layer (Dense Layer): These layers are traditional neural network layers that take the output of the previous layers and produce the final classification or regression output.

### Visualizing feature maps after convolution and pooling operations

We will try to visualize the features in each layer of a CNN using the MNIST dataset
"""

#make sure libraries like TensorFlow, Keras, NumPy, and Matplotlib are installed
#if not, uncomment the following line
#pip install tensorflow keras numpy matplotlib

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.utils import to_categorical
from keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix

"""Next, we load the MNIST dataset and view a few samples from it"""

'''
1. Load the MNSIT dataset
2. Reshape the images to (-1,28,28,1) and normalize the pixel values
    - Convolutional layers in CNNs are designed to process 2D spatial data with a channel dimension
    (e.g., grayscale images have 1 channel, and RGB images have 3 channels).
    The MNIST dataset contains 28x28 grayscale images, so the channel dimension is 1
    - Pixel values in the MNIST dataset range from 0 to 255.
    Dividing by 255.0 scales these values to the range [0, 1].

3. Display few sample images
'''
# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data() #Your Code Here

# Reshape and normalize the data
x_train = x_train.reshape((-1, 28, 28, 1))
x_test = x_test.reshape((-1, 28, 28, 1))

# Normalize the Data
x_train = x_train / 255.0 # Your code here
x_test = x_test / 255.0 # Your code here

# Display a few sample images
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')
plt.show()

"""Now, let us visualize how convolution and pooling operations affect the sizes and structure of feature maps. We load an image, create a 5x5 receptive field (convolution kernel), and perform convolution on the image. After convolution, we apply max-pooling with a 2x2 neighborhood to obtain the pooled feature map"""

image = x_train[0]

#Create a random 5x5 receptive field
receptive_field = np.random.rand(5, 5) #Your Code Here

 # Output feature map size after convolution (28-5+1)x(28-5+1)
feature_map = np.zeros((24, 24)) # Initialize it with zeros

for i in range(24):
    for j in range(24):
        feature_map[i, j] = np.sum(image[i:i+5, j:j+5, 0] * receptive_field) #Your Code Here #Perform convolution

#Perform max-pooling with a 2x2 neighborhood
pooled_feature_map = np.zeros((12, 12))

for i in range(0, 24, 2):
    for j in range(0, 24, 2):
        pooled_feature_map[i//2, j//2] = np.max(feature_map[i:i+2, j:j+2]) # Your Code Here # Perform Max pooling

#Visualize the image, feature map, and pooled feature map
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.title("Original Image")
plt.imshow(image[:, :, 0], cmap='gray')
plt.subplot(1, 3, 2)
plt.title("Feature Map")
plt.imshow(feature_map, cmap='gray')
plt.subplot(1, 3, 3)
plt.title("Pooled Feature Map")
plt.imshow(pooled_feature_map, cmap='gray')
plt.show()

"""Let us visualize how an input image propagates through the CNN and visualize the feature maps at different layers of the network. To do so, we create a simple CNN model with 2 convolutional layers and pooling."""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model  # Explicit import
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input

#Select a random MNIST image from the training set
random_index = np.random.randint(0, len(x_train))
img = x_train[random_index]
plt.title('Input img')
plt.imshow(img,cmap='gray')

#Create a simple CNN model with two convolutional layers and pooling
# First layer - A convolutional Layer with 16 kernels, 5x5 each. Remember that the input shape is (28,28,1)
# Second layer - MaxPooling2D
# Third layer - A convolutional Layer with 32 kernels, 5x5 each.
# Fourth layer - MaxPooling2D

# Your Code here
input_layer = Input(shape=(28, 28, 1))  # Explicitly define the input layer
x = Conv2D(16, (5, 5), activation='relu', name='conv2d_1')(input_layer)
x = MaxPooling2D((2, 2), name='max_pooling2d_1')(x)
x = Conv2D(32, (5, 5), activation='relu', name='conv2d_2')(x)
x = MaxPooling2D((2, 2), name='max_pooling2d_2')(x)
model = Model(inputs=input_layer, outputs=x)  # Properly create the model

img = img.reshape((28, 28, 1))  # Add the channel dimension
img = img / 255.0  # Normalize the pixel values to [0, 1]
img = np.expand_dims(img, axis=0)  # Add the batch dimension

#Create a model to visualize feature maps
layer_outputs = [layer.output for layer in model.layers]
visualization_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)

# Get the feature maps for the example image
feature_maps = visualization_model.predict(img) # Your code here

# Visualize the feature maps
layer_names = ['conv2d_1', 'max_pooling2d_1', 'conv2d_2', 'max_pooling2d_2']

# Iterate through each layer and its corresponding feature maps
for layer_name, feature_map in zip(layer_names, feature_maps):
    # Get the number of feature maps in the current layer
    n_features = feature_map.shape[-1]

    # Get the size of each feature map (assuming they are square)
    size = feature_map.shape[1]

    # Create an empty grid to display all feature maps in this layer
    display_grid = np.zeros((size, size * n_features))

    # Iterate through each feature map in the current layer
    for i in range(n_features):
        # Extract the current feature map
        x = feature_map[0, :, :, i]

        # Normalize the values for better visualization
        x -= x.mean()  # Subtract the mean value to center around 0
        x /= x.std()  # Divide by standard deviation for scaling
        x *= 64  # Scale values for better visibility
        x += 128  # Shift values to be within the [0, 255] range
        x = np.clip(x, 0, 255).astype('uint8')  # Clip values to the [0, 255] range

        # Add the current feature map to the display grid
        display_grid[:, i * size: (i + 1) * size] = x

    # Set the scale for displaying the grid
    scale = 20. / n_features

    # Create a new figure to display the feature maps for this layer
    plt.figure(figsize=(scale * n_features, scale))

    # Set the title for this layer, along with input and output size
    plt.title(f'{layer_name}\nOutput Size: {size}x{size}x{n_features}')

    # Remove grid lines for a cleaner visualization
    plt.grid(False)

    # Display the feature maps in the current layer using the viridis colormap
    plt.imshow(display_grid, aspect='auto', cmap='gray')

# Show all the visualizations
plt.show()

"""### Training a CNN

We add fully connected dense layers to the above model after flattening the feature maps. These dense layers will learn to classify the digit images after the convolutional and pooling layers extract features.

We one-hot encode the labels, preparing them for categorical cross-entropy loss during training. Next, the model is compiled with the Adam optimizer, categorical cross-entropy loss, and accuracy as the evaluation metric.

The model is trained on the training set for 5 epochs with a batch size of 128.

We plot the training history, including training and validation loss, and training and validation accuracy, to visualize how these metrics change over time.
"""

#Flatten the feature maps and add fully connected dense layers
# model.add(Flatten())
# model.add(Dense(64, activation='relu'))
# model.add(Dense(10, activation='softmax'))
#Flatten the output of the Functional model
flat = Flatten()(model.output)
# Add dense layers to the flattened output
dense1 = Dense(64, activation='relu')(flat)
output_layer = Dense(10, activation='softmax')(dense1)

# Create a new Functional model
model = Model(inputs=model.input, outputs=output_layer)

#encode the labels(one-hot)
y_train = to_categorical(y_train, num_classes=10) # Your code here
y_test = to_categorical(y_test, num_classes=10) # Your code here


#Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

#Train the model
history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=128)

#Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Over Time')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy Over Time')

plt.show()

#Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")


#Generate predictions
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

#Generate classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_true_classes, y_pred_classes))

print("\nConfusion Matrix:")
print(confusion_matrix(y_true_classes, y_pred_classes))

"""The performance analysis on each digit class for both the training and test sets reveals a crucial aspect: the CNN achieved similar accuracy on both datasets. This indicates a successful training process and robust generalization to unseen digits. The network's ability to perform well on the test set demonstrates that it didn't overfit the training data.

We can also view the learnt kernels after the model has been trained. These are shown for the first layer below.
"""

# first_layer_weights = model.layers[0].get_weights()[0]
first_layer_weights = model.get_layer('conv2d_1').get_weights()[0]

#Visualize the weights
plt.figure(figsize=(12, 6))
for i in range(16):
    plt.subplot(4, 4, i + 1)
    plt.imshow(first_layer_weights[:, :, 0, i], cmap='gray')
    plt.axis('off')
    plt.title(f'Kernel {i + 1}')

plt.suptitle('First Layer Kernels', fontsize=16)
plt.show()

"""Let us visualize a test image as it propagates through the trained model."""

#Select a random MNIST image from the test set
random_index = np.random.randint(0, len(x_test))
test_image = x_test[random_index]

#Preprocess the test image for visualization
test_image = test_image / 255.0
test_image = np.expand_dims(test_image, axis=0)

#Extract the parameters from the trained model and create a visualization model with these
layer_outputs = [layer.output for layer in model.layers]
visualization_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)

#Get the feature maps for the test image
feature_maps = visualization_model.predict(test_image)

#Visualize the feature maps at different layers
layer_names = ['conv2d_1', 'max_pooling2d_1', 'conv2d_2', 'max_pooling2d_2']

for layer_name, feature_map in zip(layer_names, feature_maps):
    # Get the number of feature maps in the current layer
    n_features = feature_map.shape[-1]

    # Create a new figure to display the feature maps for this layer
    plt.figure(figsize=(16, 4))

    # Visualize feature maps for each kernel in the current layer
    for i in range(n_features):
        feature_map_i = feature_map[0, :, :, i]

        # Normalize the values for better visualization
        feature_map_i -= feature_map_i.mean()  # Subtract the mean value to center around 0
        feature_map_i /= feature_map_i.std()  # Divide by standard deviation for scaling
        feature_map_i *= 64  # Scale values for better visibility
        feature_map_i += 128  # Shift values to be within the [0, 255] range
        feature_map_i = np.clip(feature_map_i, 0, 255).astype('uint8')  # Clip values to the [0, 255] range

        # Create subplots for each feature map
        plt.subplot(1, n_features, i + 1)
        plt.imshow(feature_map_i, cmap='gray')
        plt.axis('off')
        plt.title(f'K.{i + 1}')

    plt.suptitle(f'Layer {layer_name}', fontsize=16)
    plt.show()

"""## a. Based on the above outputs, comment on the use of the various layers of the CNN model. What features do they capture ? Why ?
Convolutional Layers:

The first convolutional layer (conv2d_1) captures low-level features like edges, corners, and lines. These features are crucial for identifying basic structures within the image.

The second convolutional layer (conv2d_2) captures more abstract and complex patterns, such as parts of digits (loops, curves, intersections). These patterns provide richer representations of the image.

Pooling Layers:

The pooling layers (max_pooling2d_1 and max_pooling2d_2) reduce the spatial dimensions of the feature maps. This helps retain the most prominent features while reducing computational complexity and overfitting.

Pooling also introduces slight invariance to translation, which is important for recognizing digits regardless of small shifts.

Fully Connected Layers:

The fully connected layers (Flatten, Dense) act as classifiers. After flattening the high-dimensional features, these layers combine the features learned by convolution and pooling to classify the image into one of the 10 digit classes.

Why They Work:

Convolutional layers focus on spatial relationships within the image, making them excellent for capturing local patterns.
Pooling layers emphasize the most important features, improving model efficiency.
Fully connected layers map the extracted features to specific classes, enabling precise classification.

## b. Take a specific image from the 'Train' set and demonstrate that it was classificed correctly.
"""

# Take a specific image from the training set
specific_index = 10  # Change this index to test a different image
specific_image = x_train[specific_index]
specific_label = np.argmax(y_train[specific_index])  # True label

# Predict using the model
predicted_label = np.argmax(model.predict(specific_image.reshape(1, 28, 28, 1)))

# Display the image and prediction
plt.imshow(specific_image.reshape(28, 28), cmap='gray')
plt.title(f"True Label: {specific_label}, Predicted: {predicted_label}")
plt.axis('off')
plt.show()

# Verify if it was classified correctly
if specific_label == predicted_label:
    print("The image was classified correctly.")
else:
    print("The image was misclassified.")

"""## c.Take an image of a your own handwritten digit (possibly in a 5cm x 5cm sheet) and verify if the model is able to classify it correctly. If it fails, justify the reason."""

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# Load the custom handwritten digit image
custom_image_path = 'digit.png'  # Replace with the image path
custom_image = Image.open(custom_image_path).convert('L')  # Convert to grayscale
custom_image = custom_image.resize((28, 28))  # Resize to 28x28 pixels

# Normalize the image to [0, 1]
custom_image = np.array(custom_image).astype('float32') / 255.0
custom_image = custom_image.reshape(1, 28, 28, 1)  # Add batch and channel dimensions

# Display the preprocessed image
plt.imshow(custom_image.reshape(28, 28), cmap='gray')
plt.title("Preprocessed Handwritten Digit")
plt.axis('off')
plt.show()

# Predict the digit using the model
custom_prediction = np.argmax(model.predict(custom_image))

# Display the predicted result
print(f"The predicted digit is: {custom_prediction}")

"""
When I tested my handwritten digit, the model predicted it incorrectly, and I think there are a few reasons why this might have happened:

1.Image Quality and Preprocessing

First, the way the image is prepared before being fed to the model really matters.

Resize and Normalize:

I resized my image to 28x28 pixels using custom_image.resize((28, 28)), which is the correct size for the model. But I need to make sure the resizing doesn’t distort the digit—stretching or squishing it can confuse the model. Also, I normalized the pixel values to the range [0, 1] using np.array(custom_image).astype('float32') / 255.0, which matches the model's expectations. So, that part seems fine.

Background and Contrast:

The model is trained on MNIST, where digits are dark and the background is light and clean. If my image has shadows, uneven lighting, or a noisy background, that could be why the model struggled. I need to make sure my digit stands out clearly against a light, uniform background.

2.Writing Style and Variations

The way I write my digit could also be an issue.

Uncommon Writing Style:

MNIST digits are written in a pretty standard style. If my handwriting is different—like if I write "7" with a loop or "4" with an extra flourish—the model might not recognize it. I need to stick to a simpler, more standard style for now.

Ambiguous Digits:

If my digit isn’t written clearly—like if strokes overlap, it’s too faint, or it’s not properly closed—the model might have trouble identifying it. Humans can often guess what it’s supposed to be, but the model needs clean, well-defined digits to work properly. I’ll try to write my digits more clearly and avoid anything fancy.

3.Model Limitations

Lastly, the model itself might have limitations.

Generalization:

The model was trained on MNIST, so it might not generalize well to digits it hasn’t seen before. If my handwriting style is too different from MNIST, the model might misclassify it. To fix this, the model would need to be retrained on a more diverse dataset.

Training Data:

MNIST only has a limited set of digit styles. If my handwriting doesn’t match the patterns in MNIST, the model won’t recognize it. Training the model on additional handwritten datasets, like EMNIST, could help it handle real-world handwriting better.

"""